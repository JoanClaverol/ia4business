{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimicazión de tareas en un almacén de comercio electrònico\n",
    "\n",
    "#### Misión\n",
    "\n",
    "Nuestra misión es construir una IA que siempre tome la ruta más corta a la ubicación de máxima prioridad, sea cual sea la ubicación desde la que comienza, y tener la opción de ir a una ubicación intermedia que se encuentre entre las 3 prioridades principales.\n",
    "\n",
    "#### Entorno\n",
    "\n",
    "Un entorno siempre requiere definir tres elementos:\n",
    "\n",
    "* **LOS ESTADOS**: ¿Dónde está nuestro robot autónomo en el almacén en cada momento *t*?\n",
    "\n",
    "Los datos de entrada en el modelo. En este caso se refiere a la posición actual donde se encuentra el robot. \n",
    "Tenderemos que transformar las letras del mapa a números, lo que se llama una codificación. \n",
    "Vamos a usar crear una matriz de recompensas, por este motivo trasnforaremos las posiciones de letras a valores numéricos. \n",
    "En este caso, la localización inicial del robot autónomo serà el valor de entrada.  \n",
    "\n",
    "* **LAS ACCIONES**: ¿Cuáles son las acciones que un robot puede realizar para ir de un punto A a un punto B?\n",
    "\n",
    "Las acciones dicen que tipo de moviemiento o hacia donde me debo dirigir. \n",
    "La acción implica moverme a un estado. Las acciones llevaran a estados y estos a acciones etc etc etc. \n",
    "No podremos saber las acciones, sin el valor de entrada. \n",
    "\n",
    "* Las recompensas: tenemos que definir una función de recompensa R que toma como entradas un estado *s* y una acción *a*, y devuelve una recompensa numérica que la IA obtendrá al llevar a cabo la acción *a* en el estado *s*.\n",
    "\n",
    "Es una combinación del estado actual en el que me encuentro y la acción que ejecuto. \n",
    "Como tenemos un número finito de estados como de acciones, declararemos las recompensas en forma de matriz; cada una de las filas se corresponde en el estado en que me encuentro mientras que cada una de las columnas se corresponde a la acción que ejecuto. \n",
    "Por lo tanto, podemos encontrar casos donde el estado sea 0 pero la acción sea 1, dado que no podemos físicamente saltar de una localización del almacén a otra, justo al lado del otro. Por lo tanto el valor sería 0. \n",
    "Ejemplos; en la fila de la A, la única opción es ir de la A a la B, por lo tanto toda la fila serán 0 excepto a la columna B, que será 1. Lo mismo con las siguientes. \n",
    "Una vez que tengamos todas las columnas completadas, podemos empezar a poner pesos. Para las acciones que representen ir a G, podemos poner un valor elevadíssimo(por ejemplo 1000), e ir decreciendo hasta llegar a la última opción. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qué es el reinforcement learning?\n",
    "\n",
    "#### La equación de Bellman\n",
    "\n",
    "Conceptos definidos por Richard Ernes Bellman: \n",
    "* s - Estado\n",
    "* a - Acción\n",
    "* R - Recompensa\n",
    "* $\\gamma$ - Factor de descuento (de momento no damos definición)\n",
    "\n",
    "Este matemático definió las bases de la programación dinámica (1953).\n",
    "\n",
    "$V(s) = max(R(s, a) + \\gamma V(s'))$\n",
    "\n",
    "Esta ecuación representa el valor en cierto estado. \n",
    "La $s'$ representa el valor del estado que va a tomar justo después de tomar cierta acción. \n",
    "El máximo representa que, de todas las posibles acciones que se puedan tomar, tomar la que maximize la recompensa ($R$). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El proceso de decisión de Markov (MDP)\n",
    "\n",
    "Hay dos formas de solucionar el problema: \n",
    "\n",
    "* Búsqueda determinista \n",
    "* Búsqueda no determinista\n",
    "\n",
    "Proceso de Markov: donde quiero ir no depende de como he llegado aquí, sino a donde estoy y hacia donde quiero ir. No depende de los estadps anteriores. \n",
    "\n",
    "Proceso de decision de Markov (MDPs) pseudo aleatorio, un entorno matemático que nos permite. \n",
    "MOvernos por el entorno no esta defini\n",
    "\n",
    "\n",
    "¿Mundo estocástico? Las mismas acciones no siempre llevan a los mismos resultados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Living penalty\n",
    " \n",
    "Centrado en el tema del reinforcement learning, el reward o recompensa. \n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "Habla sobre que tan buena és la decisión que ha tomado el agente. Q puede representar calidad de la decisión. \n",
    "\n",
    "http://ai.berkeley.edu/reinforcement.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
